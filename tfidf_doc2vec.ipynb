{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf-doc2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N8ebQIHQncj2DlsDKEy6spzW0lCLQme0",
      "authorship_tag": "ABX9TyMJUtINNvrqsXLMA1BbApVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysfArslan1/tf-idf/blob/main/tfidf_doc2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3piPhD1VQ2-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from gensim import utils\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34VHiBX1fGkF",
        "outputId": "600abfca-8f5b-4515-faf9-9dbf33744231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words =  set(stopwords.words('english'))\n",
        "stemmer = nltk.porter.PorterStemmer()"
      ],
      "metadata": {
        "id": "O_p7V1qZfBCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_wordlist(review, remove_stopwords=True):\n",
        "    # Clean the text, with the option to remove stopwords.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    words = review.lower().split()\n",
        "\n",
        "    # Optionally remove stop words (true by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    \n",
        "    review_text = \" \".join(words)\n",
        "\n",
        "    # Clean the text\n",
        "    review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\'s\", \" 's \", review_text)\n",
        "    review_text = re.sub(r\"\\'ve\", \" 've \", review_text)\n",
        "    review_text = re.sub(r\"n\\'t\", \" 't \", review_text)\n",
        "    review_text = re.sub(r\"\\'re\", \" 're \", review_text)\n",
        "    review_text = re.sub(r\"\\'d\", \" 'd \", review_text)\n",
        "    review_text = re.sub(r\"\\'ll\", \" 'll \", review_text)\n",
        "    review_text = re.sub(r\",\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\.\", \" \", review_text)\n",
        "    review_text = re.sub(r\"!\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\(\", \" ( \", review_text)\n",
        "    review_text = re.sub(r\"\\)\", \" ) \", review_text)\n",
        "    review_text = re.sub(r\"\\?\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
        "    \n",
        "    words = review_text.split()\n",
        "    \n",
        "    # Shorten words to their stems\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    \n",
        "    review_text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(review_text)"
      ],
      "metadata": {
        "id": "XALEix7adwyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data(path):\n",
        "  veriler = []\n",
        "  pdfler = os.listdir(path)\n",
        "  for pdf in pdfler:\n",
        "    try:\n",
        "        veri= {\n",
        "            \"isim\":\"\",\n",
        "        }\n",
        "        veri.update({\"isim\" : pdf})\n",
        "        dosya = open(path+pdf,\"r\")\n",
        "        for metin in dosya:\n",
        "            veri.update({\"metin\":metin})\n",
        "        \n",
        "        dosya.close()\n",
        "        veriler.append(veri)\n",
        "    except:\n",
        "        print( \"hata\")\n",
        "  return veriler"
      ],
      "metadata": {
        "id": "hqJn5KJsiJD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_groundtruth = \"/content/drive/MyDrive/kodlamalar/sevinc_hoca/nlp/Dataset/groundturth.txt\"\n",
        "dosya = open(path_groundtruth,\"r\")\n",
        "sonuc = \"\"\n",
        "for metin in dosya:\n",
        "  sonuc = metin\n",
        "dosya.close()\n",
        "groundtruth = metin"
      ],
      "metadata": {
        "id": "ZoyJ6gf3221D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_manu= \"/content/drive/MyDrive/kodlamalar/sevinc_hoca/nlp/Dataset/manuscripts/\"\n",
        "path_revi= \"/content/drive/MyDrive/kodlamalar/sevinc_hoca/nlp/Dataset/reviewers/\"\n",
        "\n",
        "manuscripts_veriler = make_data(path_manu)\n",
        "revievers_veriler = make_data(path_revi)\n"
      ],
      "metadata": {
        "id": "F5fZRFLdd_Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metin = manuscripts_veriler[0][\"metin\"]\n",
        "print(metin)\n",
        "metin = review_to_wordlist(metin,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYRJv6a6efBj",
        "outputId": "ae51ad2a-7725-42ff-b51d-c715c4855510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "massless and massive higher spins from anti-de sitter space waveguide higgs mechanism to massive higher-spin gauge fields is an outstanding openproblem. we investigate this issue in the context of kaluza-kleincompactification. starting from a free massless higher-spin field in$(d+2)$-dimensional anti-de sitter space and compactifying over a finiteangular wedge, we obtain an infinite tower of heavy, light and masslesshigher-spin fields in $(d+1)$-dimensional anti-de sitter space. all massivehigher-spin fields are described gauge invariantly in terms of st\\\"ueckelbergfields. the spectrum depends on the boundary conditions imposed at both ends ofthe wedges. we obseved that higher-derivative boundary condition is inevitablefor spin greater than three. for some higher-derivative boundary conditions,equivalently, spectrum-dependent boundary conditions, we get a non-unitaryrepresentation of partially-massless higher-spin fields of varying depth. wepresent intuitive picture which higher-derivative boundary conditions yieldnon-unitary system in terms of boundary action. we argue that isotropiclifshitz interfaces in $o(n)$ heisenberg magnet or $o(n)$ gross-neveu modelprovides the holographic dual conformal field theory and propose experimentaltest of (inverse) higgs mechanism for massive and partially masslesshigher-spin fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0LKHVj9ewGb",
        "outputId": "ee15aa92-fc99-4ece-c780-493866bc070c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "massless massiv higher spin anti de sitter space waveguid higg mechan massiv higher spin gaug field outstand openproblem investig issu context kaluza kleincompactif start free massless higher spin field in ( d 2 ) dimension anti de sitter space compactifi finiteangular wedg obtain infinit tower heavi light masslesshigh spin field ( d 1 ) dimension anti de sitter space massivehigh spin field describ gaug invari term st ueckelbergfield spectrum depend boundari condit impos end ofth wedg obsev higher deriv boundari condit inevitablefor spin greater three higher deriv boundari condit equival spectrum depend boundari condit get non unitaryrepresent partial massless higher spin field vari depth wepres intuit pictur higher deriv boundari condit yieldnon unitari system term boundari action argu isotropiclifshitz interfac o ( n ) heisenberg magnet o ( n ) gross neveu modelprovid holograph dual conform field theori propos experimentaltest ( invers ) higg mechan massiv partial masslesshigh spin field\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(manuscripts_veriler)):\n",
        "  metin = manuscripts_veriler[i][\"metin\"]\n",
        "  metin = review_to_wordlist(metin,True)\n",
        "  manuscripts_veriler[i].update({\"metin\":metin})"
      ],
      "metadata": {
        "id": "cayiipebgl6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(revievers_veriler)):\n",
        "  metin = revievers_veriler[i][\"metin\"]\n",
        "  metin = review_to_wordlist(metin,True)\n",
        "  revievers_veriler[i].update({\"metin\":metin})"
      ],
      "metadata": {
        "id": "7dakaiwvjMnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "kV3T0ZGjlHpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(text1, reviwers):\n",
        "    score = []\n",
        "    for i in range(len(reviwers)):\n",
        "      dic = {\n",
        "          \"isim\":\"\",\n",
        "          \"result\":0\n",
        "      }\n",
        "      text2 = reviwers[i][\"metin\"]\n",
        "      tfidf = vectorizer.fit_transform([text1, text2])\n",
        "      deger = ((tfidf * tfidf.T).A)[0,1]\n",
        "      dic.update({\"isim\":reviwers[i][\"isim\"]})\n",
        "      dic.update({\"result\":deger})\n",
        "      score.append(dic)\n",
        "    return score"
      ],
      "metadata": {
        "id": "6yoELh9BjpPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(manuscripts_veriler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROuaCgbPxdCx",
        "outputId": "5178e46f-5138-45b4-f129-20f3a1ae4ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "705"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_scores_deneme = []\n",
        "for i in range(10):\n",
        "    veri={\n",
        "        \"isim\":\"\",\n",
        "        \"karsilastirma_sonucları\":[]\n",
        "    }\n",
        "    score = cosine_sim(manuscripts_veriler[i][\"metin\"], revievers_veriler)\n",
        "    print(i , \" \", manuscripts_veriler[i][\"isim\"])\n",
        "    veri.update({\"isim\":manuscripts_veriler[i][\"isim\"]})\n",
        "    veri.update({\"karsilastirma_sonucları\":score})\n",
        "    Tfidf_scores_deneme.append(veri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S-A7aGqzft3",
        "outputId": "5e2645b1-49fe-4968-f03b-ac6d66a50b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   1605.06526.txt\n",
            "1   1607.05360.txt\n",
            "2   1403.7583.txt\n",
            "3   1511.07071.txt\n",
            "4   1603.06997.txt\n",
            "5   1604.07571.txt\n",
            "6   1402.5070.txt\n",
            "7   1605.08502.txt\n",
            "8   1508.01016.txt\n",
            "9   1605.02590 (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "makale = Tfidf_scores_deneme[0][\"isim\"]\n",
        "karsilastirma_sonucları = Tfidf_scores_deneme[0][\"karsilastirma_sonucları\"]\n",
        "db = pd.DataFrame(karsilastirma_sonucları)"
      ],
      "metadata": {
        "id": "J2qR5G3R0TIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#help(db.sort_values)"
      ],
      "metadata": {
        "id": "aMgEuGgq1rs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = db.sort_values('result',ascending=False ,ignore_index = True )"
      ],
      "metadata": {
        "id": "FUgi_Ncn1ePV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(makale)\n",
        "print(db.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvn3wiqL05o7",
        "outputId": "f76ed8de-b53a-48e6-f176-0a350843b620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1605.06526.txt\n",
            "                   isim    result\n",
            "0         wu, m. w..txt  0.207700\n",
            "1     kang, hyesung.txt  0.166137\n",
            "2  wolfe, arthur m..txt  0.120323\n",
            "3       ryu, dongsu.txt  0.119028\n",
            "4      jones, t. w..txt  0.119028\n",
            "5       frank, adam.txt  0.119028\n",
            "6      beck, rainer.txt  0.115232\n",
            "7      ekers, r. d..txt  0.109985\n",
            "8      dolag, klaus.txt  0.107582\n",
            "9      matsuura, m..txt  0.105843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dizi = groundtruth.split(\"'\")\n",
        "print(\"1++++++++++++++++++\",dizi[0])\n",
        "print(\"2++++++++++++++++++\",dizi[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awmg8gE3539L",
        "outputId": "f0502c68-6b15-4c51-caf8-d22d203ce8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1++++++++++++++++++ gr-qc;0701012.txt\trezzolla, luciano\tboyanovsky, d.\tsola, joan\tkalogera, v.\thu, b. l.\tmelia, fulvio\tszydlowski, marek\tcumming, a.\tamelino-camelia, giovanni\tsouradeep, tarun\tsaharian, a. a.\tshibata, masaru\ttsujikawa, shinji\tberti, emanuele\tyunes, nicolas\tshapiro, stuart l.\tsathyaprakash, b. s.\tli, baojiu\tmelatos, a.\tkumar, p.\tjones, d. i.\tvecchio, a.\tallen, b.\to\n",
            "2++++++++++++++++++ shaughnessy, r.\tsiemens, x.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bvGpCz-M4L27",
        "outputId": "1b71edb5-3ea9-4acc-9951-d1f1298da45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gr-qc;0701012.txt\\trezzolla, luciano\\tboyanovsky, d.\\tsola, joan\\tkalogera, v.\\thu, b. l.\\tmelia, fulvio\\tszydlowski, marek\\tcumming, a.\\tamelino-camelia, giovanni\\tsouradeep, tarun\\tsaharian, a. a.\\tshibata, masaru\\ttsujikawa, shinji\\tberti, emanuele\\tyunes, nicolas\\tshapiro, stuart l.\\tsathyaprakash, b. s.\\tli, baojiu\\tmelatos, a.\\tkumar, p.\\tjones, d. i.\\tvecchio, a.\\tallen, b.\\to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_scores = []\n",
        "for i in range(len(manuscripts_veriler)):\n",
        "    veri={\n",
        "        \"isim\":\"\",\n",
        "        \"karsilastirma_sonucları\":[]\n",
        "    }\n",
        "    score = cosine_sim(manuscripts_veriler[i][\"metin\"], revievers_veriler)\n",
        "    print(i , \" \", manuscripts_veriler[i][\"isim\"])\n",
        "    veri.update({\"isim\":manuscripts_veriler[i][\"isim\"]})\n",
        "    veri.update({\"karsilastirma_sonucları\":score})\n",
        "    Tfidf_scores.append(veri)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "jNAammQnkBev",
        "outputId": "eebc3a73-7c99-43f3-bae8-c0e6ef9bbe89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   1605.06526.txt\n",
            "1   1607.05360.txt\n",
            "2   1403.7583.txt\n",
            "3   1511.07071.txt\n",
            "4   1603.06997.txt\n",
            "5   1604.07571.txt\n",
            "6   1402.5070.txt\n",
            "7   1605.08502.txt\n",
            "8   1508.01016.txt\n",
            "9   1605.02590 (1).txt\n",
            "10   1605.03906.txt\n",
            "11   1601.00320.txt\n",
            "12   1606.00621 (1).txt\n",
            "13   1606.00621.txt\n",
            "14   1412.5028.txt\n",
            "15   1607.05703.txt\n",
            "16   1604.01267.txt\n",
            "17   1201.3343.txt\n",
            "18   1603.02676.txt\n",
            "19   1602.03237.txt\n",
            "20   1601.05386.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-fb9a7c527822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"karsilastirma_sonucları\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     }\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanuscripts_veriler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevievers_veriler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanuscripts_veriler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mveri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"isim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmanuscripts_veriler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-d653dbfa526f>\u001b[0m in \u001b[0;36mcosine_sim\u001b[0;34m(text1, reviwers)\u001b[0m\n\u001b[1;32m      7\u001b[0m       }\n\u001b[1;32m      8\u001b[0m       \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviwers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mdeger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"isim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mreviwers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1349\u001b[0m             )\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mmap_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}